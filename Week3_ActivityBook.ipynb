{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important: Setup Instructions\n",
    "\n",
    "**Before you begin:**\n",
    "\n",
    "1. Click **\"Copy to Drive\"** in the top menu to save your own copy\n",
    "2. Your changes will be saved to your Google Drive\n",
    "3. The original notebook will remain unchanged\n",
    "\n",
    "**Note:** The first code cell may take 10-15 seconds to run (Colab initialization). This is normal.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Activity Book: Data Preprocessing\n\n**Rysera STEM AI Course**\n\n---\n\n## Learning Objectives\n\nBy the end of this activity book, you will be able to:\n\n1. Load and explore a real-world dataset using pandas\n2. Identify and handle missing values, duplicates, and inconsistent formats\n3. Apply data transformation techniques including ordinal and one-hot encoding\n4. Create new features through feature engineering\n5. Understand the importance of data preprocessing in machine learning\n\n---\n\n## Dataset Description\n\nThis activity book uses a dataset collected from A/L students through a Google Form survey. The dataset contains information about:\n\n| Column | Description |\n|--------|-------------|\n| Timestamp | Survey submission date and time |\n| Average A/L MCQ Marks | Student marks out of 50 |\n| Number of A/L Attempts | 1st, 2nd, or 3rd Attempt |\n| Social media platforms | Platforms used (multi-select) |\n| Hours on social media | Daily social media usage |\n| Hours on self-study | Daily self-study hours |\n| Tuition class types | Type of tuition for each subject |\n| Years of past papers | Past papers completed |\n| School attendance | Attendance percentage |\n| Relationship status | Yes/No |\n| Stress level | Very Low to Very High |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 1: Loading and Initial Exploration\n",
    "\n",
    "Before we can preprocess data, we need to load it into our environment. In this section, you will learn how to:\n",
    "- Import necessary libraries\n",
    "- Load a CSV file\n",
    "- Perform initial data inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.1: Import Required Libraries\n",
    "\n",
    "Run the cell below to import the pandas library, which is essential for data manipulation in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas library\nimport pandas as pd\nimport numpy as np\n\nprint(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download validation module (run this first)\nimport urllib.request\nimport importlib\nimport sys\n\n# Always download fresh copy to avoid caching issues\nurl = 'https://raw.githubusercontent.com/KusalPabasara/ai-course-week3-activitybook/master/validator.py'\nurllib.request.urlretrieve(url, 'validator.py')\nprint('Validator module downloaded.')\n\n# Remove old cached module if exists\nif 'validator' in sys.modules:\n    del sys.modules['validator']\n\nfrom validator import check_exercise_1_1, check_assessment_1, check_assessment_2, check_exercise_4_1\nprint('Validation functions loaded.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.2: Load the Dataset\n",
    "\n",
    "Use the `pd.read_csv()` function to load the student survey data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\ndf = pd.read_csv('https://raw.githubusercontent.com/KusalPabasara/ai-course-week3-activitybook/master/5_6327612263058382043.csv')\n\n# Display the first 5 rows\nprint(\"First 5 rows of the dataset:\")\ndf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.3: Basic Dataset Information\n",
    "\n",
    "**Task:** Complete the code below to display basic information about the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the shape of the dataset (rows, columns)\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of rows: {df.shape[0]}\")\n",
    "print(f\"Number of columns: {df.shape[1]}\")\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Display column names\n",
    "print(\"Column names:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"  {i}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 1.4: Data Types and Memory Usage\n",
    "\n",
    "**Task:** Use the `.info()` method to understand the data types and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display detailed information about the dataset\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\"*50)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1.1: Answer the following questions\n",
    "\n",
    "Based on the output above, answer these questions:\n",
    "\n",
    "1. How many total entries (rows) are in the dataset?\n",
    "2. Are there any columns with missing values? If yes, which ones?\n",
    "3. What is the data type of most columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1.1: Type your answers below and run this cell\n\nanswer_1 = ___    # How many total entries (rows) are in the dataset?\nanswer_2 = \"___\"  # Are there columns with missing values? (yes/no)\nanswer_3 = \"___\"  # What is the data type of most columns? (object/int64/float64)\n\n# Check your answers\ncheck_exercise_1_1(answer_1, answer_2, answer_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 2: Data Exploration\n",
    "\n",
    "In this section, you will explore the dataset in detail to understand its structure and content before cleaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.1: Statistical Summary\n",
    "\n",
    "**Task:** Use the `.describe()` method to get statistical information about numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary for all columns (including non-numeric)\n",
    "print(\"Statistical Summary:\")\n",
    "print(\"=\"*50)\n",
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.2: Unique Values Count\n",
    "\n",
    "**Task:** Count the unique values in each column to understand the variety of responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count unique values in each column\n",
    "print(\"Unique values per column:\")\n",
    "print(\"=\"*50)\n",
    "for col in df.columns:\n",
    "    unique_count = df[col].nunique()\n",
    "    print(f\"{col}: {unique_count} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 2.3: Examining Categorical Columns\n",
    "\n",
    "**Task:** Examine the distribution of values in key categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value counts for 'Number of A/L Attempts'\n",
    "print(\"Distribution of A/L Attempts:\")\n",
    "print(df['Number of A/L Attempts'].value_counts())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Value counts for stress level\n",
    "print(\"Distribution of Stress Levels:\")\n",
    "stress_col = 'On average, how would you rate your stress level during the A/L preparation period?'\n",
    "print(df[stress_col].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2.1: Explore Another Column\n",
    "\n",
    "**Task:** Write code to display the value counts for the \"Hours on self-study\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here: Display value counts for self-study hours\n",
    "study_col = 'Approximately how many hours per day do you spend on self-study (outside of school/tuition)?'\n",
    "\n",
    "# Complete the code below:\n",
    "# print(df[______].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 3: Data Cleaning\n",
    "\n",
    "Data cleaning is the process of identifying and correcting errors in the dataset. Common issues include:\n",
    "- Missing values\n",
    "- Duplicate rows\n",
    "- Inconsistent formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.1: Identifying Missing Values\n",
    "\n",
    "**Task:** Find and count missing values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values per column\n",
    "print(\"Missing values per column:\")\n",
    "print(\"=\"*50)\n",
    "missing_counts = df.isnull().sum()\n",
    "missing_counts = missing_counts[missing_counts > 0]\n",
    "\n",
    "if len(missing_counts) == 0:\n",
    "    print(\"No missing values found.\")\n",
    "else:\n",
    "    print(missing_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.2: Examining the MCQ Marks Column\n",
    "\n",
    "The \"Average A/L MCQ Marks\" column may have inconsistent formats. Let us examine it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the MCQ marks column\n",
    "marks_col = 'Average A/L MCQ Marks (Out of 50)'\n",
    "\n",
    "print(\"Sample values from MCQ Marks column:\")\n",
    "print(df[marks_col].head(20).tolist())\n",
    "print(\"\\nData type:\", df[marks_col].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.3: Cleaning Inconsistent Formats\n",
    "\n",
    "**Task:** Clean the MCQ marks column by:\n",
    "1. Removing trailing characters (like \".\" or spaces)\n",
    "2. Converting to numeric type\n",
    "3. Handling non-numeric entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for cleaning\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Clean the MCQ marks column\n",
    "marks_col = 'Average A/L MCQ Marks (Out of 50)'\n",
    "\n",
    "# Step 1: Strip whitespace and remove trailing periods\n",
    "df_clean[marks_col] = df_clean[marks_col].astype(str).str.strip().str.rstrip('.')\n",
    "\n",
    "# Step 2: Convert to numeric (non-numeric values become NaN)\n",
    "df_clean[marks_col] = pd.to_numeric(df_clean[marks_col], errors='coerce')\n",
    "\n",
    "# Display results\n",
    "print(\"After cleaning:\")\n",
    "print(f\"Data type: {df_clean[marks_col].dtype}\")\n",
    "print(f\"Missing values: {df_clean[marks_col].isnull().sum()}\")\n",
    "print(f\"\\nBasic statistics:\")\n",
    "print(df_clean[marks_col].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.4: Handling Missing Values\n",
    "\n",
    "**Task:** Fill missing values in the MCQ marks column with the median value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate median\n",
    "median_marks = df_clean[marks_col].median()\n",
    "print(f\"Median MCQ marks: {median_marks}\")\n",
    "\n",
    "# Fill missing values with median\n",
    "df_clean[marks_col] = df_clean[marks_col].fillna(median_marks)\n",
    "\n",
    "# Verify no missing values remain\n",
    "print(f\"Missing values after filling: {df_clean[marks_col].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 3.5: Checking for Duplicates\n",
    "\n",
    "**Task:** Identify and handle duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate rows\n",
    "duplicate_count = df_clean.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# If duplicates exist, display them\n",
    "if duplicate_count > 0:\n",
    "    print(\"\\nDuplicate rows:\")\n",
    "    print(df_clean[df_clean.duplicated(keep=False)])\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    print(f\"\\nRows after removing duplicates: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Clean Another Column\n",
    "\n",
    "**Task:** Check if there are any empty strings in the \"Were you in a romantic relationship\" column and examine the unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "relationship_col = 'Were you in a romantic relationship during your A/L preparation period?'\n",
    "\n",
    "# Step 1: Display unique values\n",
    "print(\"Unique values:\")\n",
    "print(df_clean[relationship_col].unique())\n",
    "\n",
    "# Step 2: Count missing or empty values\n",
    "# Complete the code:\n",
    "# empty_count = (df_clean[relationship_col] == '').sum()\n",
    "# print(f\"Empty values: {empty_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 4: Data Transformation\n",
    "\n",
    "Data transformation involves converting data from one format to another. Key techniques include:\n",
    "- **Ordinal Encoding**: Converting ordered categories to numbers\n",
    "- **One-Hot Encoding**: Converting categories to binary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 4.1: Ordinal Encoding - Stress Level\n",
    "\n",
    "The stress level column has a natural order: Very Low < Low < Moderate < High < Very High\n",
    "\n",
    "**Task:** Convert this to numeric values (1-5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the stress level column name\n",
    "stress_col = 'On average, how would you rate your stress level during the A/L preparation period?'\n",
    "\n",
    "# View current values\n",
    "print(\"Original stress level values:\")\n",
    "print(df_clean[stress_col].value_counts())\n",
    "\n",
    "# Define ordinal mapping\n",
    "stress_mapping = {\n",
    "    'Very Low': 1,\n",
    "    'Low': 2,\n",
    "    'Moderate': 3,\n",
    "    'High': 4,\n",
    "    'Very High': 5\n",
    "}\n",
    "\n",
    "# Apply ordinal encoding\n",
    "df_clean['Stress_Level_Encoded'] = df_clean[stress_col].map(stress_mapping)\n",
    "\n",
    "# Display result\n",
    "print(\"\\nAfter ordinal encoding:\")\n",
    "print(df_clean[['Stress_Level_Encoded']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 4.2: Ordinal Encoding - Study Hours\n",
    "\n",
    "**Task:** Apply ordinal encoding to the self-study hours column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the study hours column\n",
    "study_col = 'Approximately how many hours per day do you spend on self-study (outside of school/tuition)?'\n",
    "\n",
    "# View current values\n",
    "print(\"Original study hours values:\")\n",
    "print(df_clean[study_col].value_counts())\n",
    "\n",
    "# Define ordinal mapping (based on the order of hours)\n",
    "study_mapping = {\n",
    "    '0-2 hours': 1,\n",
    "    '2-4 hours': 2,\n",
    "    '4-6 hours': 3,\n",
    "    '6+ hours': 4\n",
    "}\n",
    "\n",
    "# Apply ordinal encoding\n",
    "df_clean['Study_Hours_Encoded'] = df_clean[study_col].map(study_mapping)\n",
    "\n",
    "# Display result\n",
    "print(\"\\nAfter ordinal encoding:\")\n",
    "print(df_clean[['Study_Hours_Encoded']].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 4.3: One-Hot Encoding - A/L Attempts\n",
    "\n",
    "The \"Number of A/L Attempts\" column has no natural order. We use one-hot encoding.\n",
    "\n",
    "**Task:** Convert A/L attempts to binary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the attempts column\n",
    "attempts_col = 'Number of A/L Attempts'\n",
    "\n",
    "# View current values\n",
    "print(\"Original attempt values:\")\n",
    "print(df_clean[attempts_col].value_counts())\n",
    "\n",
    "# Apply one-hot encoding using pandas get_dummies\n",
    "attempts_encoded = pd.get_dummies(df_clean[attempts_col], prefix='Attempt')\n",
    "\n",
    "# Display the encoded columns\n",
    "print(\"\\nOne-hot encoded columns:\")\n",
    "print(attempts_encoded.head(10))\n",
    "\n",
    "# Add encoded columns to dataframe\n",
    "df_clean = pd.concat([df_clean, attempts_encoded], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 4.4: Encoding Social Media Hours\n",
    "\n",
    "**Task:** Apply ordinal encoding to the social media usage hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the social media hours column\n",
    "social_col = 'Approximately how many hours per day do you spend on social media?'\n",
    "\n",
    "# View current values\n",
    "print(\"Original social media hours values:\")\n",
    "print(df_clean[social_col].value_counts())\n",
    "\n",
    "# Define ordinal mapping\n",
    "social_mapping = {\n",
    "    'None': 0,\n",
    "    'Less than 1 hour': 1,\n",
    "    '1-3 hours': 2,\n",
    "    '3-5 hours': 3,\n",
    "    'Over 5 hours': 4\n",
    "}\n",
    "\n",
    "# Apply ordinal encoding\n",
    "df_clean['Social_Media_Hours_Encoded'] = df_clean[social_col].map(social_mapping)\n",
    "\n",
    "# Display result\n",
    "print(\"\\nAfter ordinal encoding:\")\n",
    "print(df_clean[['Social_Media_Hours_Encoded']].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4.1: Apply Ordinal Encoding\n",
    "\n",
    "**Task:** Create an ordinal encoding for the \"School attendance percentage\" column.\n",
    "\n",
    "The categories are: Below 50%, 50-75%, 75-90%, Above 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "attendance_col = 'What was your average school attendance percentage during the A/L period?'\n",
    "\n",
    "# Step 1: View current values\n",
    "print(\"Original attendance values:\")\n",
    "print(df_clean[attendance_col].value_counts())\n",
    "\n",
    "# Step 2: Define ordinal mapping (complete the dictionary)\n",
    "attendance_mapping = {\n",
    "    'Below 50%': 1,\n",
    "    '50-75%': 2,\n",
    "    # Add the remaining mappings:\n",
    "    # '75-90%': ?,\n",
    "    # 'Above 90%': ?\n",
    "}\n",
    "\n",
    "# Step 3: Apply encoding\n",
    "# df_clean['Attendance_Encoded'] = df_clean[attendance_col].map(attendance_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 5: Feature Engineering\n",
    "\n",
    "Feature engineering is the process of creating new features from existing data to improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 5.1: Parse Multi-Select Column\n",
    "\n",
    "The \"Social media platforms\" column contains multiple values separated by semicolons.\n",
    "\n",
    "**Task:** Create a feature that counts the number of platforms used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the social media platforms column\n",
    "platforms_col = 'Which social media platforms do you use?'\n",
    "\n",
    "# View sample values\n",
    "print(\"Sample values:\")\n",
    "print(df_clean[platforms_col].head(5).tolist())\n",
    "\n",
    "# Create feature: count of platforms\n",
    "df_clean['Platform_Count'] = df_clean[platforms_col].str.split(';').str.len()\n",
    "\n",
    "# Display results\n",
    "print(\"\\nPlatform count distribution:\")\n",
    "print(df_clean['Platform_Count'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 5.2: Create Binary Features from Multi-Select\n",
    "\n",
    "**Task:** Create binary features for specific popular platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary features for common platforms\n",
    "df_clean['Uses_WhatsApp'] = df_clean[platforms_col].str.contains('WhatsApp', na=False).astype(int)\n",
    "df_clean['Uses_YouTube'] = df_clean[platforms_col].str.contains('YouTube', na=False).astype(int)\n",
    "df_clean['Uses_Instagram'] = df_clean[platforms_col].str.contains('Instagram', na=False).astype(int)\n",
    "df_clean['Uses_TikTok'] = df_clean[platforms_col].str.contains('TikTok', na=False).astype(int)\n",
    "\n",
    "# Display results\n",
    "print(\"Binary platform features:\")\n",
    "print(df_clean[['Uses_WhatsApp', 'Uses_YouTube', 'Uses_Instagram', 'Uses_TikTok']].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 5.3: Extract Day of Week from Timestamp\n",
    "\n",
    "**Task:** Extract the day of the week from the submission timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime\n",
    "df_clean['Timestamp'] = pd.to_datetime(df_clean['Timestamp'], format='mixed')\n",
    "\n",
    "# Extract day of week (0=Monday, 6=Sunday)\n",
    "df_clean['Submission_Day'] = df_clean['Timestamp'].dt.dayofweek\n",
    "\n",
    "# Map to day names\n",
    "day_names = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', \n",
    "             4: 'Friday', 5: 'Saturday', 6: 'Sunday'}\n",
    "df_clean['Submission_Day_Name'] = df_clean['Submission_Day'].map(day_names)\n",
    "\n",
    "# Display results\n",
    "print(\"Submissions by day of week:\")\n",
    "print(df_clean['Submission_Day_Name'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 5.4: Create Composite Feature\n",
    "\n",
    "**Task:** Create a \"Study Intensity\" score by combining study hours and stress level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create study intensity score (study hours * stress level)\n",
    "df_clean['Study_Intensity'] = df_clean['Study_Hours_Encoded'] * df_clean['Stress_Level_Encoded']\n",
    "\n",
    "# Display results\n",
    "print(\"Study Intensity distribution:\")\n",
    "print(df_clean['Study_Intensity'].describe())\n",
    "print(\"\\nValue counts:\")\n",
    "print(df_clean['Study_Intensity'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.1: Create Your Own Feature\n",
    "\n",
    "**Task:** Create a new feature called \"Digital_Engagement\" that combines platform count and social media hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "# Hint: You can add or multiply Platform_Count with Social_Media_Hours_Encoded\n",
    "\n",
    "# Complete the code:\n",
    "# df_clean['Digital_Engagement'] = df_clean['Platform_Count'] + df_clean['Social_Media_Hours_Encoded']\n",
    "\n",
    "# Display results:\n",
    "# print(df_clean['Digital_Engagement'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Section 6: Review and Summary\n",
    "\n",
    "Let us review all the transformations we have applied to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 6.1: View Final Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display all columns in the cleaned dataset\n",
    "print(\"Final dataset columns:\")\n",
    "print(\"=\"*50)\n",
    "for i, col in enumerate(df_clean.columns, 1):\n",
    "    print(f\"{i}. {col}\")\n",
    "\n",
    "print(f\"\\nTotal columns: {len(df_clean.columns)}\")\n",
    "print(f\"Total rows: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity 6.2: View New Features Only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the new columns we created\n",
    "new_columns = ['Stress_Level_Encoded', 'Study_Hours_Encoded', 'Social_Media_Hours_Encoded',\n",
    "               'Platform_Count', 'Uses_WhatsApp', 'Uses_YouTube', 'Uses_Instagram', 'Uses_TikTok',\n",
    "               'Submission_Day', 'Study_Intensity']\n",
    "\n",
    "# Add attempt columns if they exist\n",
    "attempt_cols = [col for col in df_clean.columns if col.startswith('Attempt_')]\n",
    "new_columns.extend(attempt_cols)\n",
    "\n",
    "print(\"New features created through preprocessing:\")\n",
    "print(\"=\"*50)\n",
    "df_clean[new_columns].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Assessment Questions\n",
    "\n",
    "Answer the following questions to test your understanding of data preprocessing concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Fill in the Blanks\n",
    "\n",
    "Complete the following statements:\n",
    "\n",
    "1. Data preprocessing consists of three main steps: Data _______, Data Integration, and Data _______.\n",
    "\n",
    "2. _______ encoding is used when categorical values have a natural order.\n",
    "\n",
    "3. _______ encoding creates binary columns for each category.\n",
    "\n",
    "4. The pandas method to convert non-numeric strings to NaN is `pd.to_numeric(data, errors='_______')`.\n",
    "\n",
    "5. The method `.______()` is used to fill missing values in pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessment Question 1: Fill in the blanks\n# Type your answers as strings (single word)\n\nblank_1 = \"___\"  # Data preprocessing step 1: Data _______\nblank_2 = \"___\"  # Data preprocessing step 3: Data _______\nblank_3 = \"___\"  # _______ encoding is used when categories have natural order\nblank_4 = \"___\"  # One-_______ encoding creates binary columns for each category\nblank_5 = \"___\"  # pd.to_numeric(data, errors='_______') converts invalid to NaN\n\n# Check your answers\ncheck_assessment_1(blank_1, blank_2, blank_3, blank_4, blank_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Code Completion\n",
    "\n",
    "Complete the code to perform ordinal encoding on a \"Priority\" column with values: Low, Medium, High"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assessment Question 2: Complete the ordinal encoding\nsample_df = pd.DataFrame({\"Priority\": [\"High\", \"Low\", \"Medium\", \"High\", \"Low\"]})\n\n# Complete the mapping dictionary\npriority_mapping = {\n    \"Low\": 1,\n    \"Medium\": ___,  # Fill in the value\n    \"High\": ___     # Fill in the value\n}\n\n# Check your answer\nif check_assessment_2(priority_mapping):\n    sample_df[\"Priority_Encoded\"] = sample_df[\"Priority\"].map(priority_mapping)\n    print(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: Short Answer\n",
    "\n",
    "Answer the following questions in your own words:\n",
    "\n",
    "1. Why is data preprocessing important in machine learning?\n",
    "\n",
    "2. When would you choose ordinal encoding over one-hot encoding?\n",
    "\n",
    "3. What are three common \"garbage\" issues found in real-world datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your answers here as comments\n",
    "# Question 1:\n",
    "# \n",
    "# Question 2:\n",
    "# \n",
    "# Question 3:\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this activity book, you learned:\n",
    "\n",
    "1. **Data Loading**: How to load CSV files and inspect dataset structure\n",
    "2. **Data Exploration**: Using `.info()`, `.describe()`, and value counts\n",
    "3. **Data Cleaning**: Handling missing values, duplicates, and inconsistent formats\n",
    "4. **Data Transformation**: Applying ordinal and one-hot encoding\n",
    "5. **Feature Engineering**: Creating new features from existing data\n",
    "\n",
    "These preprocessing steps are essential before applying any machine learning algorithm to your data.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Activity Book**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}